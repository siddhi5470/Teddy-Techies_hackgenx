# -*- coding: utf-8 -*-
"""HACKGENX

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r6a1zfYVfdoXL7G2RNosmyLEUHzRBpaU
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
# Load the dataset
file_path = '/content/Medical_Device_Failure_dataset.csv'
df = pd.read_csv(file_path)

print(df.columns)

#Finding missing values in %
missing_percentage = df.isnull().sum() / len(df) * 100
print("Percentage of missing values:\n", missing_percentage)

for column in df.columns:
    unique_values = df[column].unique()
    print(f"Column: {column}")
    print(f"First 6 Unique Values: {unique_values[:6]}")
    print(f"Total Unique Values: {len(unique_values)}\n")

"""
# **Converting Categorical to Numerical**
```

"""

# Load the original ordinal encoded dataset
df = pd.read_csv("/content/Medical_Device_Failure_dataset.csv")

# Convert 'Purchase_Date' to datetime
purchase_date_numeric = pd.to_datetime(df['Purchase_Date'])
reference_date = purchase_date_numeric.min()
purchase_date_numeric = (purchase_date_numeric - reference_date).dt.days

# Create a new dataset with numeric 'Purchase_Date'
df_updated = df.copy()
df_updated['Purchase_Date'] = purchase_date_numeric

# Save the updated dataset with numeric dates
df_updated.to_csv("/content/date_normalized_dataset.csv", index=False)

# Optional: Preview
print(df_updated.head())

# Load the dataset
file_path = "/content/date_normalized_dataset.csv"
df = pd.read_csv(file_path)

categorical_columns = [
    "Device_ID", "Device_Type", "Manufacturer", "Model", "Country",
    "Maintenance_Report"
]

# Target variable
target_column = "Maintenance_Frequency"

# Convert target column to numeric codes if it's categorical
df[target_column] = df[target_column].astype('category')
df["Maintenance_Frequency"] = df[target_column].cat.codes

# Create a copy of the dataset
df_encoded = df.copy()

# Apply Target Encoding
for col in categorical_columns:
    mean_target = df.groupby(col)["Maintenance_Frequency"].mean()
    df_encoded[col] = df[col].map(mean_target).round().astype(int)

# Save the encoded dataset
encoded_file_path = "target_encoded_dataset.csv"
df_encoded.to_csv(encoded_file_path, index=False)

print(f"Target encoded dataset saved at: {encoded_file_path}")

from sklearn.preprocessing import OrdinalEncoder

# Load your dataset
df = pd.read_csv("/content/date_normalized_dataset.csv")

# Define the columns to encode
columns_to_encode = ["Device_ID", "Device_Type", "Manufacturer", "Model", "Country","Maintenance_Report"]
# Initialize OrdinalEncoder
encoder = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)

# Apply Ordinal Encoding directly to selected columns (in-place)
df[columns_to_encode] = encoder.fit_transform(df[columns_to_encode])
df[columns_to_encode] = df[columns_to_encode].abs()

# Print the modified DataFrame
print(df.head())

# Save the transformed dataset (Optional)
df.to_csv("ordinal encoded_dataset.csv", index=False)

from sklearn.preprocessing import MinMaxScaler

# Load your dataset
df = pd.read_csv("/content/ordinal encoded_dataset.csv")

# Columns to normalize
columns_to_normalize = ["Device_ID", "Purchase_Date", "Maintenance_Cost", "Downtime"]

# Initialize and apply MinMaxScaler
scaler = MinMaxScaler()
df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])

# Save or view the result
print(df.head())
# Optionally save to a new file
df.to_csv("normalized_dataset.csv", index=False)

"""#  Exploratory Data Analytics"""

import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv("/content/Medical_Device_Failure_dataset.csv")

# Set plot style
sns.set(style="whitegrid")
plt.figure(figsize=(16, 12))

# 1. Asset Health Distribution (Failure Event Count)
plt.subplot(2, 2, 1)
sns.histplot(df['Failure_Event_Count'], bins=10, kde=True, color="skyblue")
plt.title("Asset Health Distribution (Failure Event Count)")
plt.xlabel("Failure Event Count")
plt.ylabel("Number of Devices")

# 2. Frequency of Manufacturing by Manufacturer
plt.subplot(2, 2, 2)
manufacturer_counts = df['Manufacturer'].value_counts()
sns.barplot(x=manufacturer_counts.index, y=manufacturer_counts.values, palette="viridis")
plt.title("Frequency of Manufacturing by Manufacturer")
plt.xlabel("Manufacturer")
plt.ylabel("Number of Devices")
plt.xticks(rotation=45)

# 3. Average Life Cycle by Manufacturer (Age)
plt.subplot(2, 2, 4)
avg_lifecycle = df.groupby("Manufacturer")["Age"].mean().sort_values(ascending=False)
sns.barplot(x=avg_lifecycle.index, y=avg_lifecycle.values, palette="cubehelix")
plt.title("Average Life Cycle by Manufacturer")
plt.xlabel("Manufacturer")
plt.ylabel("Average Age (Years)")
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x="Device_Type", y="Failure_Event_Count", palette="cool")
plt.title("Failure Events by Device Type")
plt.xlabel("Device Type")
plt.ylabel("Failure Event Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.heatmap(df.select_dtypes(include='number').corr(), annot=True, cmap='Blues')
plt.title("Correlation Heatmap of Numeric Features")
plt.tight_layout()
plt.show()

"""# **Feature Selection**

# **1.Dispersiv Flies Optimization(DFO)**
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load dataset
file_path = "target_encoded_dataset.csv"
df = pd.read_csv(file_path)

# Drop ID column if present
if 'ID' in df.columns:
    df = df.drop(columns=['Device_ID'])

# Define features and target
target_column = "Maintenance_Report"
X = df.drop(columns=['Maintenance_Report'])
y = df[target_column]

num_features = X.shape[1]  # Total number of features
num_flies = 10  # Population size
iterations = 10

# DFO Parameters
gamma = 0.5
delta = 0.1

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize flies randomly (binary values 0 or 1)
population = np.random.choice([0, 1], size=(num_flies, num_features))

# Fitness function: Evaluate feature subset using classification accuracy
def fitness(solution):
    selected_features = np.where(solution == 1)[0]
    if len(selected_features) == 0:
        return 0
    clf = RandomForestClassifier()
    clf.fit(X_train.iloc[:, selected_features], y_train)
    y_pred = clf.predict(X_test.iloc[:, selected_features])
    return accuracy_score(y_test, y_pred)

# Main DFO function
def dispersive_flies_optimization():
    global population

    for t in range(iterations):
        # Evaluate fitness
        fitness_values = np.array([fitness(fly) for fly in population])

        # Find the best fly
        best_index = np.argmax(fitness_values)
        best_fly = population[best_index].copy()

        # Update each fly
        for i in range(num_flies):
            if i == best_index:
                continue  # Skip best fly

            for j in range(num_features):
                if np.random.rand() < gamma:
                    population[i, j] = best_fly[j]  # Move towards the best fly
                if np.random.rand() < delta:
                    population[i, j] = 1 - population[i, j]

        # Slightly perturb the best fly for better exploration
        for j in range(num_features):
            if np.random.rand() < delta:
                best_fly[j] = 1 - best_fly[j]
        population[best_index] = best_fly

        print(f"Iteration {t+1}: Best Accuracy = {fitness(best_fly):.4f}")

# Run DFO optimization
dispersive_flies_optimization()

# Final selected feature subset
final_fly = population[np.argmax([fitness(fly) for fly in population])]
selected_feature_indices = np.where(final_fly == 1)[0]
selected_feature_names = X.columns[selected_feature_indices].tolist()

if len(selected_feature_names) == 0:
    print("\nNo features were selected. Exiting.")
else:
    print("\nFinal Selected Features:", selected_feature_names)

# Create new dataset with selected features and target column
new_dataset = df[selected_feature_names + [target_column]]
new_dataset.to_csv("DFO_1.csv", index=False)
print("\nNew dataset saved as 'DFO_1.csv'")

import time

# Load dataset
file_path = "/content/normalized_dataset.csv"
df = pd.read_csv(file_path)

# Drop ID column if present
if 'ID' in df.columns:
    df = df.drop(columns=['Device_ID'])

# Define features and target
target_column = "Maintenance_Report"
X = df.drop(columns=['Maintenance_Report'])
y = df[target_column]

num_features = X.shape[1]
num_flies = 10
iterations = 10

# DFO Parameters
gamma = 0.5
delta = 0.1

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize flies randomly (binary values 0 or 1)
population = np.random.choice([0, 1], size=(num_flies, num_features))

# Fitness function: Evaluate feature subset using classification accuracy
def fitness(solution):
    selected_features = np.where(solution == 1)[0]
    if len(selected_features) == 0:
        return 0
    clf = RandomForestClassifier()
    clf.fit(X_train.iloc[:, selected_features], y_train)
    y_pred = clf.predict(X_test.iloc[:, selected_features])
    return accuracy_score(y_test, y_pred)

# Max allowed runtime in seconds
max_runtime = 600
start_time = time.time()

# Main DFO function with runtime limit
def dispersive_flies_optimization():
    global population

    for t in range(iterations):
        elapsed_time = time.time() - start_time
        if elapsed_time > max_runtime:
            print("\n Time limit reached! Stopping early.")
            break

        # Evaluate fitness
        fitness_values = np.array([fitness(fly) for fly in population])

        # Find the best fly
        best_index = np.argmax(fitness_values)
        best_fly = population[best_index].copy()

        # Update each fly
        for i in range(num_flies):
            if i == best_index:
                continue  # Skip best fly

            for j in range(num_features):
                if np.random.rand() < gamma:
                    population[i, j] = best_fly[j]
                if np.random.rand() < delta:
                    population[i, j] = 1 - population[i, j]

        # Slightly perturb the best fly for better exploration
        for j in range(num_features):
            if np.random.rand() < delta:
                best_fly[j] = 1 - best_fly[j]
        population[best_index] = best_fly

        print(f"Iteration {t+1}: Best Accuracy = {fitness(best_fly):.4f}")

# Run DFO optimization
dispersive_flies_optimization()

# Final selected feature subset
final_fly = population[np.argmax([fitness(fly) for fly in population])]
selected_feature_indices = np.where(final_fly == 1)[0]

# Fix: Extract correct feature names
selected_feature_names = X.columns[selected_feature_indices].tolist()

if len(selected_feature_names) == 0:
    print("\nNo features were selected. Exiting.")
else:
    print("\nFinal Selected Features:", selected_feature_names)

    # Create new dataset with selected features and target column
    new_dataset = df[selected_feature_names + [target_column]]
    new_dataset.to_csv("DFO_2.csv", index=False)
    print("\nNew dataset saved as 'DFO_2.csv'")

"""# **2.Fish School Search (FSS)**"""

from sklearn. preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import random

# Load dataset
dataset_path = "target_encoded_dataset.csv"
df = pd.read_csv(dataset_path)

# Define target variable
target_column = "Maintenance_Report"
X = df.drop(columns=[target_column])
y = df[target_column]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Fish School Search (FSS) Parameters
num_fish = 10  # Number of fish
num_features = X_train.shape[1]  # Total features
iterations = 10  # Max iterations
initial_step_size = 2  # Initial step size
step_size = initial_step_size
max_runtime = 300  # Maximum runtime in seconds (5 minutes)

# Timer start
start_time = time.time()

def evaluate_fitness(subset):
    selected_features = np.where(subset == 1)[0]
    if len(selected_features) == 0:
        return 0  # Avoid empty feature sets
    X_train_sel = X_train[:, selected_features]
    X_test_sel = X_test[:, selected_features]
    clf = LogisticRegression(max_iter=1000)
    clf.fit(X_train_sel, y_train)
    predictions = clf.predict(X_test_sel)
    return accuracy_score(y_test, predictions)

# Initialize fish population
fish_population = np.random.randint(0, 2, (num_fish, num_features))
weights = np.ones(num_fish)  # Initial weights
fitness = np.array([evaluate_fitness(fish) for fish in fish_population])

def compute_barycenter(fish_population, weights):
    weighted_sum = np.sum(fish_population * weights[:, np.newaxis], axis=0)
    total_weight = np.sum(weights)
    return (weighted_sum / total_weight) > 0.5

# Main FSS loop
for iter in range(iterations):
    iteration_start_time = time.time()

    # Stop execution if max runtime is exceeded
    if time.time() - start_time > max_runtime:
        print("\nMaximum runtime exceeded! Stopping optimization.")
        break

    # Step 1: Individual Movement
    random_values = np.random.uniform(-1, 1, (num_fish, num_features))
    flip_mask = (np.abs(random_values) * step_size) > 1
    new_population = np.where(flip_mask, 1 - fish_population, fish_population)
    new_fitness = np.array([evaluate_fitness(fish) for fish in new_population])

    # Step 2: Feeding & Weight Update
    delta_fitness = new_fitness - fitness
    weights += delta_fitness
    weights = np.clip(weights, 0.1, None)

    # Step 3: Update fish positions if fitness improves
    improved_mask = delta_fitness > 0
    fish_population[improved_mask] = new_population[improved_mask]
    fitness[improved_mask] = new_fitness[improved_mask]

    # Step 4: Compute Barycenter (Collective Movement)
    best_fish = compute_barycenter(fish_population, weights)
    for i in range(num_fish):
        if random.random() < 0.5:
            fish_population[i] = best_fish

    # Step 5: Reduce Step Size Gradually
    step_size *= 0.99

    iteration_end_time = time.time()  # End iteration timer
    iteration_time = iteration_end_time - iteration_start_time

    print(f"Iteration {iter+1}: Best Accuracy = {max(fitness):.4f}, Time Taken = {iteration_time:.2f} seconds")

# Final best subset
best_fish = fish_population[np.argmax(fitness)]
selected_feature_indices = np.where(best_fish == 1)[0]

# Fix: Correctly extract feature names from df.columns
selected_feature_names = X.columns[selected_feature_indices].tolist()

if len(selected_feature_names) == 0:
    print("\nNo features were selected. Exiting.")
else:
    print("\nBest selected features:", selected_feature_names)

    # Create New Dataset with Selected Features
    new_df = df[selected_feature_names + [target_column]]
    new_df.to_csv('FSS_1.csv', index=False)
    print("\nNew dataset with selected features saved as 'FSS_1.csv'")

import numpy as np
import pandas as pd
import random
import time
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Load dataset
dataset_path = "/content/normalized_dataset.csv"
df = pd.read_csv(dataset_path)

# Define target variable
target_column = "Maintenance_Report"
X = df.drop(columns=[target_column])
y = df[target_column]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Fish School Search (FSS) Parameters
num_fish = 10  # Number of fish
num_features = X_train.shape[1]  # Total features
iterations = 10  # Max iterations
initial_step_size = 2  # Initial step size
step_size = initial_step_size
max_runtime = 300  # Maximum runtime in seconds (5 minutes)

# Timer start
start_time = time.time()

def evaluate_fitness(subset):
    selected_features = np.where(subset == 1)[0]
    if len(selected_features) == 0:
        return 0  # Avoid empty feature sets
    X_train_sel = X_train[:, selected_features]
    X_test_sel = X_test[:, selected_features]
    clf = LogisticRegression(max_iter=1000)
    clf.fit(X_train_sel, y_train)
    predictions = clf.predict(X_test_sel)
    return accuracy_score(y_test, predictions)

# Initialize fish population
fish_population = np.random.randint(0, 2, (num_fish, num_features))
weights = np.ones(num_fish)  # Initial weights
fitness = np.array([evaluate_fitness(fish) for fish in fish_population])

def compute_barycenter(fish_population, weights):
    weighted_sum = np.sum(fish_population * weights[:, np.newaxis], axis=0)
    total_weight = np.sum(weights)
    return (weighted_sum / total_weight) > 0.5  # Majority voting for feature selection

# Main FSS loop
for iter in range(iterations):
    iteration_start_time = time.time()  # Start iteration timer

    # Stop execution if max runtime is exceeded
    if time.time() - start_time > max_runtime:
        print("\nMaximum runtime exceeded! Stopping optimization.")
        break

    # Step 1: Individual Movement
    random_values = np.random.uniform(-1, 1, (num_fish, num_features))
    flip_mask = (np.abs(random_values) * step_size) > 1
    new_population = np.where(flip_mask, 1 - fish_population, fish_population)
    new_fitness = np.array([evaluate_fitness(fish) for fish in new_population])

    # Step 2: Feeding & Weight Update
    delta_fitness = new_fitness - fitness
    weights += delta_fitness
    weights = np.clip(weights, 0.1, None)  # Ensure positive weights

    # Step 3: Update fish positions if fitness improves
    improved_mask = delta_fitness > 0
    fish_population[improved_mask] = new_population[improved_mask]
    fitness[improved_mask] = new_fitness[improved_mask]

    # Step 4: Compute Barycenter (Collective Movement)
    best_fish = compute_barycenter(fish_population, weights)
    for i in range(num_fish):
        if random.random() < 0.5:
            fish_population[i] = best_fish

    # Step 5: Reduce Step Size Gradually
    step_size *= 0.99

    iteration_end_time = time.time()  # End iteration timer
    iteration_time = iteration_end_time - iteration_start_time  # Compute iteration time

    print(f"Iteration {iter+1}: Best Accuracy = {max(fitness):.4f}, Time Taken = {iteration_time:.2f} seconds")

# Final best subset
best_fish = fish_population[np.argmax(fitness)]
selected_feature_indices = np.where(best_fish == 1)[0]

# Fix: Correctly extract feature names from df.columns
selected_feature_names = X.columns[selected_feature_indices].tolist()

if len(selected_feature_names) == 0:
    print("\nNo features were selected. Exiting.")
else:
    print("\nBest selected features:", selected_feature_names)

    # Create New Dataset with Selected Features
    new_df = df[selected_feature_names + [target_column]]
    new_df.to_csv('FSS_2.csv', index=False)
    print("\nNew dataset with selected features saved as 'FSS_2.csv'")

"""# **3.Frog - Snake Prey- Predator Realtionship Optimization (FSRO)**"""

import time
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load dataset
file_path = "target_encoded_dataset.csv"
df = pd.read_csv(file_path)

# Define target and features
target_column = "Maintenance_Report"
features = [col for col in df.columns if col != target_column and col != "Device_ID"]
X = df[features]
y = df[target_column]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# FSRO Parameters
num_frogs = 10
num_features = len(features)
num_memeplexes = 2  # Dividing frogs into groups
max_iter = 10
alpha = 0.5  # Probability of moving toward the best frog
elimination_threshold = 0.2
max_runtime = 300
# Initialize frogs with random feature subsets
def initialize_frogs(num_frogs, num_features):
    frogs = np.random.randint(0, 2, (num_frogs, num_features))  # Binary feature selection
    for i in range(num_frogs):
        if np.sum(frogs[i]) == 0:
            frogs[i, np.random.randint(0, num_features)] = 1
    return frogs

# Evaluate fitness (model accuracy)
def evaluate_fitness(frogs):
    accuracies = []
    for frog in frogs:
        selected_features = [features[i] for i in range(num_features) if frog[i] == 1]
        if len(selected_features) == 0:
            accuracies.append(0)
            continue
        model = RandomForestClassifier(random_state=42)
        model.fit(X_train[selected_features], y_train)
        preds = model.predict(X_test[selected_features])
        accuracies.append(accuracy_score(y_test, preds))
    return np.array(accuracies)

# Select memeplexes
def form_memeplexes(frogs, fitness):
    sorted_indices = np.argsort(-fitness)  # Sort frogs by fitness (descending)
    memeplexes = np.array_split(frogs[sorted_indices], num_memeplexes)
    return memeplexes

# Local search within memeplexes
def local_search(memeplex):
    best_frog = memeplex[0]
    for i in range(1, len(memeplex)):
        if np.random.rand() < alpha:
            memeplex[i] = np.where(np.random.rand(num_features) < alpha, best_frog, memeplex[i])
    return memeplex

# Global movement toward the best frog
def snake_predation(frogs, best_frog):
    movement_prob = np.random.rand(*frogs.shape)  # Probability matrix for gradual movement
    return np.where(movement_prob < alpha, best_frog, frogs)

# Feature importance-guided random moves
def feature_guided_move(frog, feature_importance):
    importance_threshold = np.median(feature_importance)
    for i in range(num_features):
        if np.random.rand() < 0.3:
            if feature_importance[i] > importance_threshold:
                frog[i] = 1
            else:
                frog[i] = 0
    return frog

# FSRO Execution with Timer
frogs = initialize_frogs(num_frogs, num_features)
start_time = time.time()

for iteration in range(max_iter):
    elapsed_time = time.time() - start_time
    if elapsed_time > max_runtime:
        print("\n Time limit reached! Stopping early.")
        break

    fitness = evaluate_fitness(frogs)
    best_idx = np.argmax(fitness)
    best_frog = frogs[best_idx]

    # Form memeplexes
    memeplexes = form_memeplexes(frogs, fitness)

    # Apply local search
    for i in range(len(memeplexes)):
        elapsed_time = time.time() - start_time
        if elapsed_time > max_runtime:
            print("\n Time limit reached during iteration! Stopping early.")
            break
        memeplexes[i] = local_search(memeplexes[i])

    # Merge back the memeplexes
    frogs = np.vstack(memeplexes)

    # Global movement (snake predation)
    elapsed_time = time.time() - start_time
    if elapsed_time > max_runtime:
        print("\n Time limit reached during iteration! Stopping early.")
        break
    frogs = snake_predation(frogs, best_frog)

    # Feature importance-based guided move
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    feature_importance = model.feature_importances_
    for i in range(num_frogs):
        elapsed_time = time.time() - start_time
        if elapsed_time > max_runtime:
            print("\n Time limit reached during iteration! Stopping early.")
            break
        frogs[i] = feature_guided_move(frogs[i], feature_importance)

    # Eliminate weak frogs and replace them with new ones
    min_fitness_idx = np.argmin(fitness)
    if fitness[min_fitness_idx] < elimination_threshold:
        frogs[min_fitness_idx] = initialize_frogs(1, num_features)[0]

    print(f"Iteration {iteration + 1}: Best Accuracy = {max(fitness):.2f}")

# Final selected features
final_selected_features = [features[i] for i in range(num_features) if best_frog[i] == 1]
print("\nFinal Selected Features:", final_selected_features)

# Create New Dataset with Selected Features
new_df = df[final_selected_features + [target_column]]
new_df.to_csv('FSRO_1.csv', index=False)
print("New dataset with selected features saved as 'FSRO_1.csv'")

import time
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load dataset
file_path = "/content/normalized_dataset.csv"
df = pd.read_csv(file_path)

# Define target and features
target_column = "Maintenance_Report"
features = [col for col in df.columns if col != target_column and col != "Device_ID"]
X = df[features]
y = df[target_column]

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# FSRO Parameters
num_frogs = 10
num_features = len(features)
num_memeplexes = 2
max_iter = 10
alpha = 0.5
elimination_threshold = 0.2

max_runtime = 300

# Initialize frogs with random feature subsets
def initialize_frogs(num_frogs, num_features):
    frogs = np.random.randint(0, 2, (num_frogs, num_features))
    for i in range(num_frogs):
        if np.sum(frogs[i]) == 0:
            frogs[i, np.random.randint(0, num_features)] = 1
    return frogs

# Evaluate fitness (model accuracy)
def evaluate_fitness(frogs):
    accuracies = []
    for frog in frogs:
        selected_features = [features[i] for i in range(num_features) if frog[i] == 1]
        if len(selected_features) == 0:
            accuracies.append(0)
            continue
        model = RandomForestClassifier(random_state=42)
        model.fit(X_train[selected_features], y_train)
        preds = model.predict(X_test[selected_features])
        accuracies.append(accuracy_score(y_test, preds))
    return np.array(accuracies)

# Select memeplexes
def form_memeplexes(frogs, fitness):
    sorted_indices = np.argsort(-fitness)  # Sort frogs by fitness (descending)
    memeplexes = np.array_split(frogs[sorted_indices], num_memeplexes)
    return memeplexes

# Local search within memeplexes
def local_search(memeplex):
    best_frog = memeplex[0]
    for i in range(1, len(memeplex)):
        if np.random.rand() < alpha:
            memeplex[i] = np.where(np.random.rand(num_features) < alpha, best_frog, memeplex[i])
    return memeplex

# Global movement  toward the best frog
def snake_predation(frogs, best_frog):
    movement_prob = np.random.rand(*frogs.shape)
    return np.where(movement_prob < alpha, best_frog, frogs)

# Feature importance-guided random moves
def feature_guided_move(frog, feature_importance):
    importance_threshold = np.median(feature_importance)
    for i in range(num_features):
        if np.random.rand() < 0.3:
            if feature_importance[i] > importance_threshold:
                frog[i] = 1
            else:
                frog[i] = 0
    return frog

# FSRO Execution with Timer
frogs = initialize_frogs(num_frogs, num_features)
start_time = time.time()

for iteration in range(max_iter):
    elapsed_time = time.time() - start_time
    if elapsed_time > max_runtime:
        print("\n Time limit reached! Stopping early.")
        break
    fitness = evaluate_fitness(frogs)
    best_idx = np.argmax(fitness)
    best_frog = frogs[best_idx]

    # Form memeplexes
    memeplexes = form_memeplexes(frogs, fitness)

    # Apply local search
    for i in range(len(memeplexes)):
        elapsed_time = time.time() - start_time
        if elapsed_time > max_runtime:
            print("\n  Time limit reached during iteration! Stopping early.")
            break
        memeplexes[i] = local_search(memeplexes[i])

    # Merge back the memeplexes
    frogs = np.vstack(memeplexes)

    # Global movement
    elapsed_time = time.time() - start_time
    if elapsed_time > max_runtime:
        print("\n Time limit reached during iteration! Stopping early.")
        break
    frogs = snake_predation(frogs, best_frog)

    # Feature importance-based guided move
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    feature_importance = model.feature_importances_
    for i in range(num_frogs):
        elapsed_time = time.time() - start_time
        if elapsed_time > max_runtime:
            print("\n Time limit reached during iteration! Stopping early.")
            break
        frogs[i] = feature_guided_move(frogs[i], feature_importance)

    # Eliminate weak frogs and replace them with new ones
    min_fitness_idx = np.argmin(fitness)
    if fitness[min_fitness_idx] < elimination_threshold:
        frogs[min_fitness_idx] = initialize_frogs(1, num_features)[0]

    print(f"Iteration {iteration + 1}: Best Accuracy = {max(fitness):.2f}")

# Final selected features
final_selected_features = [features[i] for i in range(num_features) if best_frog[i] == 1]
print("\nFinal Selected Features:", final_selected_features)

# Create New Dataset with Selected Features
new_df = df[final_selected_features + [target_column]]
new_df.to_csv('FSRO_2.csv', index=False)
print("New dataset with selected features saved as 'FSRO_2.csv'")

"""# **4. Principal Component Analysis (PCA)**"""

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv("target_encoded_dataset.csv")
X = df.drop(columns=['Maintenance_Report']).values
y = df['Maintenance_Report'].values
feature_names = df.drop(columns=['Maintenance_Report']).columns

# Step 1: Standardize the data (PCA works better with normalized data)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 2: Perform PCA on the original features
pca = PCA(n_components=X.shape[1])  # Keep all components initially
X_pca = pca.fit_transform(X_scaled)

# Step 3: Analyze explained variance to decide how many components to keep
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

# Select the number of components that explain at least 95% of variance
selected_components = np.where(cumulative_variance >= 0.95)[0][0] + 1
print(f"Selected {selected_components} principal components covering 95% of variance.")

# Step 4: Identify the most important original features
pca_loadings = np.abs(pca.components_[:selected_components])  # Get absolute values of loadings
feature_importance = np.sum(pca_loadings, axis=0)  # Sum contributions per feature
selected_features_indices = np.argsort(feature_importance)[-selected_components:]  # Top features
selected_feature_names = feature_names[selected_features_indices]

# Step 5: Display selected features
print("\nSelected Features based on PCA:")
print(selected_feature_names.tolist())

# Step 6: Create new dataset with selected original features
X_selected = X[:, selected_features_indices]  # Keep only selected features
df_selected = pd.DataFrame(X_selected, columns=selected_feature_names)
df_selected['efs'] = y  # Add target column

# Step 7: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# Step 8: Train a classifier on selected features
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Step 9: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy after PCA-Based Feature Selection: {accuracy:.4f}")

# Step 10: Save the selected feature dataset
df_selected.to_csv("PCA_1.csv", index=False)
print("\nDataset with PCA-selected features saved as 'PCA_1.csv'")

import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv("/content/normalized_dataset.csv")
X = df.drop(columns=['Maintenance_Frequency']).values
y = df['Maintenance_Frequency'].values
feature_names = df.drop(columns=['Maintenance_Frequency']).columns

# Step 1: Standardize the data (PCA works better with normalized data)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 2: Perform PCA on the original features
pca = PCA(n_components=X.shape[1])  # Keep all components initially
X_pca = pca.fit_transform(X_scaled)

# Step 3: Analyze explained variance to decide how many components to keep
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

# Select the number of components that explain at least 95% of variance
selected_components = np.where(cumulative_variance >= 0.95)[0][0] + 1
print(f"Selected {selected_components} principal components covering 95% of variance.")

# Step 4: Identify the most important original features
pca_loadings = np.abs(pca.components_[:selected_components])  # Get absolute values of loadings
feature_importance = np.sum(pca_loadings, axis=0)  # Sum contributions per feature
selected_features_indices = np.argsort(feature_importance)[-selected_components:]  # Top features
selected_feature_names = feature_names[selected_features_indices]

# Step 5: Display selected features
print("\nSelected Features based on PCA:")
print(selected_feature_names.tolist())

# Step 6: Create new dataset with selected original features
X_selected = X[:, selected_features_indices]  # Keep only selected features
df_selected = pd.DataFrame(X_selected, columns=selected_feature_names)
df_selected['efs'] = y  # Add target column

# Step 7: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)

# Step 8: Train a classifier on selected features
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Step 9: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy after PCA-Based Feature Selection: {accuracy:.4f}")

# Step 10: Save the selected feature dataset
df_selected.to_csv("PCA_2.csv", index=False)
print("\nDataset with PCA-selected features saved as 'PCA_2.csv'")

import numpy as np
import matplotlib.pyplot as plt

# Define algorithm names
algorithm_names = [
    "FSRO","FSS","DFO","PCA"
]

# Replace these with actual accuracy values from your experiments
accuracy_values_targeted = [
    0.74,1.00,1.00,0.73]

accuracy_values_ordinal = [
    0.23,0.22,0.22,0.20]

# Create the bar chart
plt.figure(figsize=(12, 6))
bar_width = 0.4
indices = np.arange(len(algorithm_names))

plt.barh(indices + bar_width / 2, accuracy_values_targeted, bar_width, label='Targeted Encoding', color='skyblue')
plt.barh(indices - bar_width / 2, accuracy_values_ordinal, bar_width, label='Ordinal Encoding', color='pink')

plt.xlabel("Accuracy")
plt.title("Comparison of Feature Selection Algorithms based on Accuracy")
plt.xlim(0.0, 1.0)
plt.yticks(indices, algorithm_names)
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.legend()

# Show values on bars
for index, value in enumerate(accuracy_values_targeted):
    plt.text(value - 0.01, index + bar_width / 2, f"{value:.2f}", va='center', ha='right', fontsize=10)

for index, value in enumerate(accuracy_values_ordinal):
    plt.text(value - 0.01, index - bar_width / 2, f"{value:.2f}", va='center', ha='right', fontsize=10)

plt.gca().invert_yaxis()  # Invert y-axis to have the best accuracy at the top
plt.show()

"""# **Model Building**

# **1.XGbooster**
"""

import pandas as pd
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# Load dataset
df = pd.read_csv('/content/target_encoded_dataset.csv')

# Define target column
target_col = 'Maintenance_Report'

# Separate features and target
X = df.drop(columns=[target_col])
y = df[target_col]

# Always create a label encoder and fit to y
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Train the XGBoost classifier
model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')
model.fit(X, y_encoded)

# Predict
y_pred = model.predict(X)

# Evaluate
print(" XGBoost model trained on full dataset")
print("Accuracy:", accuracy_score(y_encoded, y_pred))
print("Classification Report:\n", classification_report(y_encoded, y_pred))

# Plot Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=model.feature_importances_, y=X.columns)
plt.title(" Feature Importances - XGBoost")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

# Save predictions
df['predicted_' + target_col] = label_encoder.inverse_transform(y_pred)
df.to_csv('xgboost_classification_output.csv', index=False)
print("Predictions saved to 'xgboost_classification_output.csv'")

device_type_encoder = LabelEncoder()
device_type_encoder.fit(df['Device_Type'])
target_col = 'Maintenance_Report'
label_encoder = LabelEncoder()
df[target_col] = label_encoder.fit_transform(df[target_col])


# Save it
joblib.dump(device_type_encoder, 'device_type_encoder.pkl')
joblib.dump(model, 'best_model.pkl')
joblib.dump(label_encoder, 'label_encoder.pkl')
joblib.dump(list(X.columns), 'feature_list.pkl')
print(" Model, label encoder, and feature list saved for future user-input prediction.")

import pandas as pd
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('/content/normalized_dataset.csv')

# Define target column
target_col = 'Maintenance_Report'

# Separate features (X) and target (y)
X = df.drop(columns=[target_col])
y = df[target_col]

# Label Encode target column if it's categorical
if y.dtype == 'object' or y.dtype.name == 'category':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Train model on the entire dataset using XGBoost
model = XGBClassifier(random_state=42)
model.fit(X, y)

# Predict on the same dataset
y_pred = model.predict(X)

# Evaluation
print(" Model trained on full numerical dataset")
print("Accuracy:", accuracy_score(y, y_pred))
print("Classification Report:\n", classification_report(y, y_pred))

# Feature importance
importances = model.feature_importances_
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=X.columns)
plt.title("Feature Importances - XGBoost")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

# Save predictions
df['predicted_' + target_col] = y_pred
df.to_csv('xgboost_classification_output.csv', index=False)
print(" Predictions saved to 'xgboost_classification_output.csv'")

"""# **Adaptive Boosting**"""

import pandas as pd
from sklearn.ensemble import AdaBoostClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('/content/target_encoded_dataset.csv')

# Define target column
target_col = 'Maintenance_Report'

# Separate features (X) and target (y)
X = df.drop(columns=[target_col])
y = df[target_col]

# Label Encode target column if it's categorical
if y.dtype == 'object' or y.dtype.name == 'category':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Initialize AdaBoostClassifier
model = AdaBoostClassifier(random_state=42)
model.fit(X, y)

# Predict on the same dataset
y_pred = model.predict(X)

# Evaluation
print(" Model trained on full numerical dataset using AdaBoost")
print("Accuracy:", accuracy_score(y, y_pred))
print("Classification Report:\n", classification_report(y, y_pred))

# Feature importance
importances = model.feature_importances_
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=X.columns)
plt.title("Feature Importances - AdaBoost")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

# Save predictions
df['predicted_' + target_col] = y_pred
df.to_csv('adaboost_classification_output.csv', index=False)
print(" Predictions saved to 'adaboost_classification_output.csv'")

import pandas as pd
from sklearn.ensemble import AdaBoostClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('/content/normalized_dataset.csv')

# Define target column
target_col = 'Maintenance_Report'

# Separate features (X) and target (y)
X = df.drop(columns=[target_col])
y = df[target_col]

# Label Encode target column if it's categorical
if y.dtype == 'object' or y.dtype.name == 'category':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Initialize AdaBoostClassifier
model = AdaBoostClassifier(random_state=42)
model.fit(X, y)

# Predict on the same dataset
y_pred = model.predict(X)

# Evaluation
print(" Model trained on full numerical dataset using AdaBoost")
print("Accuracy:", accuracy_score(y, y_pred))
print("Classification Report:\n", classification_report(y, y_pred))

# Feature importance
importances = model.feature_importances_
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=X.columns)
plt.title("Feature Importances - AdaBoost")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

# Save predictions
df['predicted_' + target_col] = y_pred
df.to_csv('adaboost_classification_output.csv', index=False)
print(" Predictions saved to 'adaboost_classification_output.csv'")

"""# **Long-Short Term Memory (LSTM)**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('/content/target_encoded_dataset.csv')

# Define target column
target_col = 'Maintenance_Report'

# Separate features and target
X = df.drop(columns=[target_col])
y = df[target_col]

# Encode target if categorical
if y.dtype == 'object' or y.dtype.name == 'category':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape input for LSTM: (samples, timesteps, features)
X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 1 timestep

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)

# Define LSTM model
model = Sequential()
model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu'))
model.add(Dense(len(np.unique(y)), activation='softmax'))  # Multiclass classification

# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# Predict
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

# Evaluation
print("LSTM Model Evaluation")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plot training history
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('LSTM Training History')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('/content/normalized_dataset.csv')

# Define target column
target_col = 'Maintenance_Report'

# Separate features and target
X = df.drop(columns=[target_col])
y = df[target_col]

# Encode target if categorical
if y.dtype == 'object' or y.dtype.name == 'category':
    le = LabelEncoder()
    y = le.fit_transform(y)

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Reshape input for LSTM: (samples, timesteps, features)
X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))  # 1 timestep

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)

# Define LSTM model
model = Sequential()
model.add(LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=False))
model.add(Dropout(0.3))
model.add(Dense(32, activation='relu'))
model.add(Dense(len(np.unique(y)), activation='softmax'))  # Multiclass classification

# Compile model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train model
history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# Predict
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)

# Evaluation
print("LSTM Model Evaluation")
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# Plot training history
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('LSTM Training History')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Define algorithm names
algorithm_names = [
    "XGBooster","AdpBoost","LSTM",
]

# Replace these with actual accuracy values from your experiments
accuracy_values_targeted = [
    0.98,0.77,0.76]

accuracy_values_ordinal = [
    0.63,0.28,0.00]

# Create the bar chart
plt.figure(figsize=(12, 6))
bar_width = 0.4
indices = np.arange(len(algorithm_names))

plt.barh(indices + bar_width / 2, accuracy_values_targeted, bar_width, label='Targeted Encoding', color='skyblue')
plt.barh(indices - bar_width / 2, accuracy_values_ordinal, bar_width, label='Ordinal Encoding', color='pink')

plt.xlabel("Accuracy")
plt.title("Comparison of Feature Selection Algorithms based on Accuracy")
plt.xlim(0.0, 1.0)
plt.yticks(indices, algorithm_names)
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.legend()

# Show values on bars
for index, value in enumerate(accuracy_values_targeted):
    plt.text(value - 0.01, index + bar_width / 2, f"{value:.2f}", va='center', ha='right', fontsize=10)

for index, value in enumerate(accuracy_values_ordinal):
    plt.text(value - 0.01, index - bar_width / 2, f"{value:.2f}", va='center', ha='right', fontsize=10)

plt.gca().invert_yaxis()
plt.show()

import pandas as pd
import joblib

# Load saved components
model = joblib.load('best_model.pkl')                    # Trained XGBoost model
label_encoder = joblib.load('label_encoder.pkl')         # LabelEncoder for target
feature_list = joblib.load('feature_list.pkl')           # Full feature list used in training
device_type_encoder = joblib.load('device_type_encoder.pkl')  # LabelEncoder for Device_Type

# Selected input features
numerical_features = ['Age', 'Maintenance_Class']
categorical_features = ['Device_Type']
target_col = 'Maintenance_Report'

# Step 1: Take input from user
def get_user_input():
    print("\n Please provide the following inputs:")
    user_data = {}

    # Get numerical features
    for feature in numerical_features:
        try:
            user_data[feature] = float(input(f"Enter value for '{feature}': "))
        except ValueError:
            print(f" Invalid input for '{feature}', defaulting to 0.")
            user_data[feature] = 0.0

    # Get and encode categorical feature
    for feature in categorical_features:
        value = input(f"Enter value for '{feature}': ")
        try:
            user_data[feature] = device_type_encoder.transform([value])[0]
        except:
            user_data[feature] = 0

    return pd.DataFrame([user_data])

# Step 2: Complete missing features
def complete_user_input(user_df):
    complete_df = pd.DataFrame(columns=feature_list)
    for col in feature_list:
        complete_df[col] = [0]  # default values
    for col in user_df.columns:
        complete_df[col] = user_df[col]
    return complete_df

# Step 3: Predict
def predict_user_input():
    user_input_df = get_user_input()
    complete_df = complete_user_input(user_input_df)
    prediction = model.predict(complete_df)
    predicted_label = label_encoder.inverse_transform(prediction)
    print(f"\n The predicted value for '{target_col}' is: {predicted_label[0]}")

# Run
predict_user_input()

import joblib
joblib.dump(model, 'asset_model.pkl')

from google.colab import files
files.download('asset_model.pkl')

